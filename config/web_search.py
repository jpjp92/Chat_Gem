# config/web_search.py
import urllib.request
import urllib.parse
import json
import re
import logging
from datetime import datetime
import uuid

logger = logging.getLogger(__name__)

class WebSearchAPI:
    def __init__(self, client_id, client_secret, cache_handler, cache_ttl=3600, daily_limit=25000):
        self.client_id = client_id
        self.client_secret = client_secret
        self.cache = cache_handler
        self.cache_ttl = cache_ttl
        self.daily_limit = daily_limit
        self.request_count = 0
        self.base_url = "https://openapi.naver.com/v1/search/webkr"
    
    def get_request_count(self):
        """í˜„ì¬ ìš”ì²­ íšŸìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return self.request_count
    
    def increment_request_count(self):
        """ìš”ì²­ íšŸìˆ˜ë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤."""
        self.request_count += 1
    
    def is_over_limit(self):
        """ì¼ì¼ í•œë„ ì´ˆê³¼ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
        return self.request_count >= self.daily_limit
    
    def search_web(self, query, display=5, sort="date"):
        """Naver APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        # ìºì‹œ í‚¤ì— ë‚ ì§œ í¬í•¨ (ë§¤ì¼ ìƒˆë¡œìš´ ê²€ìƒ‰)
        today = datetime.now().strftime("%Y-%m-%d")
        cache_key = f"naver:{query}:{display}:{sort}:{today}"
        cached = self.cache.get(cache_key)
        if cached:
            logger.info(f"ìºì‹œì—ì„œ ê²€ìƒ‰ ê²°ê³¼ ë°˜í™˜: {cache_key}")
            return cached
        
        if self.is_over_limit():
            return "ê²€ìƒ‰ í•œë„ ì´ˆê³¼ë¡œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"
        
        try:
            enc_text = urllib.parse.quote(query)
            url = f"{self.base_url}?query={enc_text}&display={display}&sort={sort}"
            
            request = urllib.request.Request(url)
            request.add_header("X-Naver-Client-Id", self.client_id)
            request.add_header("X-Naver-Client-Secret", self.client_secret)
            
            response = urllib.request.urlopen(request, timeout=3)
            self.increment_request_count()
            
            if response.getcode() == 200:
                data = json.loads(response.read().decode('utf-8'))
                results = data.get('items', [])
                
                if not results:
                    return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ğŸ˜“"
                
                formatted_result = self.format_search_results(results)
                # diskcacheëŠ” set(key, value, expire=ttl) í˜•ì‹ ì‚¬ìš©
                self.cache.set(cache_key, formatted_result, expire=self.cache_ttl)
                return formatted_result
            else:
                return f"ê²€ìƒ‰ API ì˜¤ë¥˜ (ì½”ë“œ: {response.getcode()}) ğŸ˜“"
                
        except Exception as e:
            logger.error(f"Naver API ì˜¤ë¥˜: {str(e)}")
            return "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ğŸ˜“"
    
    def format_search_results(self, results):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
        response_text = "ğŸŒ **ì›¹ ê²€ìƒ‰ ê²°ê³¼**\n\n"
        
        formatted_results = []
        for i, item in enumerate(results, 1):
            # HTML íƒœê·¸ ì œê±°
            clean_title = re.sub(r'<b>|</b>', '', item.get('title', 'ì œëª© ì—†ìŒ'))
            clean_description = re.sub(r'<b>|</b>', '', item.get('description', 'ë‚´ìš© ì—†ìŒ'))
            
            # ì„¤ëª… ê¸¸ì´ ì œí•œ (300ì)
            description_preview = clean_description[:300] + "..." if len(clean_description) > 300 else clean_description
            
            # ë””ë²„ê·¸: ì›ë³¸ description ë¡œê¹…
            logger.debug(f"ê²€ìƒ‰ ê²°ê³¼ {i}: {clean_title[:50]}... | Description ê¸¸ì´: {len(clean_description)}")
            
            formatted_result = (
                f"**{i}. {clean_title}**\n"
                f"{description_preview}\n"
                f"ğŸ”— {item.get('link', '')}"
            )
            formatted_results.append(formatted_result)
        
        response_text += "\n\n".join(formatted_results)
        
        logger.info(f"âœ… ê²€ìƒ‰ ê²°ê³¼ í¬ë§·íŒ… ì™„ë£Œ: {len(formatted_results)}ê°œ í•­ëª©")
        response_text += "\n\në” ê¶ê¸ˆí•œ ì  ìˆë‚˜ìš”? ğŸ˜Š"
        
        return response_text

    def should_search(self, query):
        """
        ê°œì„ ëœ ê²€ìƒ‰ í•„ìš” íŒë‹¨ê¸° (ìŠ¤ì½”ì–´ë§ + ë¶€ì • í‚¤ì›Œë“œ ì‹œìŠ¤í…œ)
        ë°˜í™˜: (bool, reason)
        - True: ê²€ìƒ‰ì´ í•„ìš”í•¨
        - False: ê²€ìƒ‰ ë¶ˆí•„ìš”, ì´ìœ  ë¬¸ìì—´ ë°˜í™˜
        """
        if not query or not isinstance(query, str):
            return False, "ë¹ˆ ì¿¼ë¦¬"

        q = query.lower()
        
        # ========================================
        # 0. ì§§ì€ ì¶”ê°€ ì§ˆë¬¸ í•„í„°ë§ (ì´ì „ ì»¨í…ìŠ¤íŠ¸ í™œìš©)
        # ========================================
        short_followup_patterns = [
            r'^[ê°€-í£]{1,3}[ëŠ”ì€]?\\?*$',  # "ì˜¨ë„ëŠ”?", "ìŠµë„?"
            r'^[ê°€-í£]{1,5}[ìš”ì•¼]?\\?*$',  # "ì–¼ë§ˆì•¼?", "ëª‡ë„ìš”?"
            r'^ì •í™•í•œ\\s*[ê°€-í£]{2,4}',    # "ì •í™•í•œ ì˜¨ë„"
            r'^êµ¬ì²´ì ì¸\\s*[ê°€-í£]{2,4}',  # "êµ¬ì²´ì ì¸ ìŠµë„"
            r'^\\w{1,10}\\?*$',            # ì˜ì–´ ë‹¨ì–´ í•˜ë‚˜ "temperature?"
        ]
        
        for pattern in short_followup_patterns:
            if re.search(pattern, q):
                return False, "ì¶”ê°€ ì§ˆë¬¸ (ì´ì „ ì»¨í…ìŠ¤íŠ¸ í™œìš©)"

        # ========================================
        # 1. ë¶€ì • í‚¤ì›Œë“œ ì²´í¬ (ì¼ë°˜ ì§€ì‹/ì„¤ëª… ìš”ì²­)
        # ========================================
        NEGATIVE_KEYWORDS = [
            # ì¼ë°˜ ì„¤ëª…/ì •ì˜ ìš”ì²­ (í•œêµ­ì–´)
            r"\\bì„¤ëª…í•´\\b", r"\\bì„¤ëª…\\b", r"\\bì´ë€\\b", r"\\bë­ì•¼\\b", r"\\bë¬´ì—‡\\b",
            r"\\bë­”ì§€\\b", r"\\bì˜ë¯¸\\b", r"\\bê°œë…\\b", r"\\bì •ì˜\\b",
            r"\\bì´í•´\\b", r"\\bì•Œë ¤ì¤˜\\b(?!.*ê²€ìƒ‰)", # "ì•Œë ¤ì¤˜"ë§Œ ë‹¨ë…ìœ¼ë¡œ (ê²€ìƒ‰ ì—†ì´)
            # ë¹„êµ/ì¶”ì²œ (ê²€ìƒ‰ ë¶ˆí•„ìš”)
            r"\\bì°¨ì´\\b", r"\\bë¹„êµ\\b", r"\\bì¶”ì²œ\\b", r"\\bì¢‹ì€\\b", r"\\bë‚˜ì€\\b",
            # ì˜ê²¬/ìƒê° ìš”ì²­
            r"\\bìƒê°í•´\\b", r"\\bì˜ê²¬\\b", r"\\bì–´ë–»ê²Œ ìƒê°\\b",
            # ì—­ì‚¬/ì´ë¡ /í•™ë¬¸
            r"\\bì—­ì‚¬\\b", r"\\bê¸°ì›\\b", r"\\bìœ ë˜\\b", r"\\bì´ë¡ \\b",
            r"\\bì² í•™\\b", r"\\bì›ë¦¬\\b", r"\\bê³¼í•™\\b", r"\\bìˆ˜í•™\\b",
            # ì¼ë°˜ ëŒ€í™”
            r"\\bì•ˆë…•\\b", r"\\bê°ì‚¬\\b", r"\\bê³ ë§ˆì›Œ\\b", r"\\bë¯¸ì•ˆ\\b",
            
            # ì˜ì–´
            r"\\bexplain\\b", r"\\bwhat is\\b", r"\\bwhat's\\b", r"\\bwhat are\\b",
            r"\\bdefine\\b", r"\\bdefinition\\b", r"\\bmeaning\\b", r"\\bconcept\\b",
            r"\\btell me about\\b(?!.*(latest|recent|current))", # "tell me about"ë§Œ ë‹¨ë…
            r"\\bcompare\\b", r"\\bdifference\\b", r"\\brecommend\\b",
            r"\\bopinion\\b", r"\\bthink\\b", r"\\bbelieve\\b",
            r"\\bhistory\\b", r"\\btheory\\b", r"\\borigin\\b", r"\\bphilosophy\\b",
            r"\\bhello\\b", r"\\bthanks\\b", r"\\bthank you\\b", r"\\bsorry\\b",
            
            # ìŠ¤í˜ì¸ì–´
            r"\\bexplicar\\b", r"\\bquÃ© es\\b", r"\\bcuÃ¡l es\\b",
            r"\\bdefinir\\b", r"\\bdefiniciÃ³n\\b", r"\\bsignificado\\b",
            r"\\bcomparar\\b", r"\\bdiferencia\\b", r"\\brecomendar\\b",
            r"\\bopiniÃ³n\\b", r"\\bpensar\\b", r"\\bcreer\\b",
            r"\\bhistoria\\b", r"\\bteorÃ­a\\b", r"\\borigen\\b",
            r"\\bhola\\b", r"\\bgracias\\b", r"\\bperdÃ³n\\b",
        ]
        
        for neg_pattern in NEGATIVE_KEYWORDS:
            if re.search(neg_pattern, q):
                # ë‹¨, ì‹¤ì‹œê°„ í‚¤ì›Œë“œì™€ í•¨ê»˜ ì‚¬ìš©ë˜ë©´ ê²€ìƒ‰ í—ˆìš©
                realtime_override = ['ìµœì‹ ', 'í˜„ì¬', 'ì‹¤ì‹œê°„', 'ì˜¤ëŠ˜', 'latest', 'current', 'today', 'now']
                if not any(rt in q for rt in realtime_override):
                    return False, f"ì¼ë°˜ ëŒ€í™”/ì„¤ëª… ìš”ì²­ ê°ì§€: {neg_pattern}"

        # ========================================
        # 2. ëª…ì‹œì  ê²€ìƒ‰ ì˜ë„ ì²´í¬ (ìš°ì„ ìˆœìœ„ ìµœìƒìœ„)
        # ========================================
        EXPLICIT_SEARCH_KEYWORDS = [
            # í•œêµ­ì–´
            r"\\bê²€ìƒ‰\\b", r"\\bì°¾ì•„ë´\\b", r"\\bì°¾ì•„ì¤˜\\b", r"\\bì¡°íšŒ\\b",
            r"\\bê²€ìƒ‰í•´\\b", r"\\bì•Œì•„ë´\\b",
            # ì˜ì–´
            r"\\bsearch\\b", r"\\blook up\\b", r"\\bfind out\\b",
            r"\\bgoogle\\b", r"\\bcheck\\b",
            # ìŠ¤í˜ì¸ì–´
            r"\\bbuscar\\b", r"\\bbusca\\b", r"\\bconsultar\\b",
        ]
        
        for search_kw in EXPLICIT_SEARCH_KEYWORDS:
            if re.search(search_kw, q):
                return True, f"ëª…ì‹œì  ê²€ìƒ‰ ìš”ì²­: {search_kw}"

        # ========================================
        # 3. ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ (ì‹¤ì‹œê°„ ì •ë³´ í•„ìš”ì„± í‰ê°€)
        # ========================================
        score = 0.0
        matched_reasons = []
        
        # 3.1) ì‹¤ì‹œê°„ í•„ìˆ˜ ì •ë³´ (ë†’ì€ ì ìˆ˜)
        REALTIME_CRITICAL = {
            # ë‚ ì”¨ (1.5ì )
            'ë‚ ì”¨': 1.5, 'weather': 1.5, 'tiempo': 1.5,
            'ê¸°ì˜¨': 1.5, 'temperature': 1.5, 'temperatura': 1.5,
            # ê¸ˆìœµ (1.5ì )
            'ì£¼ê°€': 1.5, 'stock': 1.5, 'bolsa': 1.5,
            'í™˜ìœ¨': 1.5, 'exchange rate': 1.5, 'tipo de cambio': 1.5,
            'ë¹„íŠ¸ì½”ì¸': 1.3, 'bitcoin': 1.3,
            # ë‰´ìŠ¤/ì†ë³´ (1.5ì )
            'ë‰´ìŠ¤': 1.5, 'news': 1.5, 'noticias': 1.5,
            'ì†ë³´': 1.5, 'breaking': 1.5,
        }
        
        for kw, points in REALTIME_CRITICAL.items():
            if kw in q:
                score += points
                matched_reasons.append(f'{kw}(+{points})')
        
        # 3.2) ì‹œê°„ì„± í‚¤ì›Œë“œ (ì¤‘ê°„ ì ìˆ˜)
        TEMPORAL_KEYWORDS = {
            'ì˜¤ëŠ˜': 1.0, 'today': 1.0, 'hoy': 1.0,
            'í˜„ì¬': 1.0, 'current': 1.0, 'actual': 1.0,
            'ì§€ê¸ˆ': 1.0, 'now': 1.0, 'ahora': 1.0,
            'ìµœì‹ ': 1.0, 'latest': 1.0, 'Ãºltimo': 1.0,
            'ìµœê·¼': 0.8, 'recent': 0.8, 'reciente': 0.8,
            'ì‹¤ì‹œê°„': 1.2, 'real-time': 1.2, 'tiempo real': 1.2,
        }
        
        for kw, points in TEMPORAL_KEYWORDS.items():
            if kw in q:
                score += points
                matched_reasons.append(f'{kw}(+{points})')
        
        # 3.3) ì˜ì•½í’ˆ (ì•ˆì „ì„ ìœ„í•´ ê²€ìƒ‰ ê¶Œì¥)
        MEDICINE_KEYWORDS = {
            'íƒ€ì´ë ˆë†€': 1.5, 'tylenol': 1.5,
            'ì•„ìŠ¤í”¼ë¦°': 1.5, 'aspirin': 1.5,
            'ë¶€ì‘ìš©': 1.3, 'side effect': 1.3, 'efectos secundarios': 1.3,
            'ë³µìš©ë²•': 1.3, 'dosage': 1.3, 'dosis': 1.3,
            'íš¨ëŠ¥': 1.0, 'efficacy': 1.0,
        }
        
        for kw, points in MEDICINE_KEYWORDS.items():
            if kw in q:
                score += points
                matched_reasons.append(f'{kw}(+{points})')
        
        # 3.4) ì§€ì—­ + ë‚ ì”¨/ì‹œê°„ ì¡°í•© (ê°•ë ¥í•œ ì‹¤ì‹œê°„ ì§€í‘œ)
        location_weather_pattern = r'(ì„œìš¸|ë¶€ì‚°|ì¸ì²œ|ë‰´ìš•|ëŸ°ë˜|ë„ì¿„|íŒŒë¦¬|ë² ì´ì§•|LA|ì‹œë“œë‹ˆ).*(ë‚ ì”¨|ê¸°ì˜¨|ì‹œê°„|ì˜¨ë„|weather|temperature)'
        if re.search(location_weather_pattern, q):
            score += 2.0
            matched_reasons.append('ì§€ì—­+ë‚ ì”¨(+2.0)')
        
        # 3.5) ë‚ ì§œ/ì—°ë„ í¬í•¨ (ì¤‘ê°„ ì ìˆ˜)
        if re.search(r'(202[0-9]|203[0-9])ë…„?', q):
            score += 0.8
            matched_reasons.append('ì—°ë„(+0.8)')
        
        if re.search(r'\\b(\\d{1,2}ì›”|ì–´ì œ|ë‚´ì¼|ë°©ê¸ˆ)\\b', q):
            score += 0.5
            matched_reasons.append('ë‚ ì§œ(+0.5)')
        
        # ========================================
        # 4. Threshold íŒë‹¨ (2.5 ì´ìƒì´ë©´ ê²€ìƒ‰)
        # ========================================
        THRESHOLD = 2.5
        
        if score >= THRESHOLD:
            reason = f"ì‹¤ì‹œê°„ ì •ë³´ í•„ìš” (ì ìˆ˜: {score:.1f}/{THRESHOLD}, ë§¤ì¹­: {', '.join(matched_reasons)})"
            logger.info(f"âœ… ê²€ìƒ‰ í—ˆìš©: {reason}")
            return True, reason
        
        # ========================================
        # 5. ê¸°ë³¸: ê²€ìƒ‰ ë¶ˆí•„ìš”
        # ========================================
        if score > 0:
            return False, f'ì ìˆ˜ ë¶€ì¡± ({score:.1f} < {THRESHOLD})'
        else:
            return False, 'ì‹¤ì‹œê°„ ì •ë³´ ë¶ˆí•„ìš” (ì¼ë°˜ ëŒ€í™”)'

    def get_function_signature(self):
        """
        LLM(Function Calling)ìš© í•¨ìˆ˜ ì„œëª…/ìŠ¤í™ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        ì´ ìŠ¤í™ì„ ëª¨ë¸ì— ì œê³µí•˜ë©´ ëª¨ë¸ì´ í•„ìš”ì‹œ í•´ë‹¹ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        return {
            "name": "web_search",
            "description": "ì›¹ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ìµœì‹ ì„±(ë‰´ìŠ¤, ì£¼ê°€, í™˜ìœ¨, ë‚ ì”¨ ë“±)ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ í˜¸ì¶œí•˜ì„¸ìš”.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¬¸ìì—´"},
                    "display": {"type": "integer", "description": "ë°˜í™˜í•  ê²°ê³¼ ìˆ˜", "default": 5},
                    "sort": {"type": "string", "description": "ì •ë ¬ ë°©ì‹ (date|sim)", "enum": ["date", "sim"], "default": "date"}
                },
                "required": ["query"]
            }
        }
    
    def search_and_create_context(self, query, session_state=None):
        """ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ì»¨í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        logger.info(f"ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        # ì¿¼ë¦¬ì—ì„œ 'ê²€ìƒ‰' í‚¤ì›Œë“œ ì œê±°
        clean_query = query.lower().replace("ê²€ìƒ‰", "").strip()
        
        # ë‚ ì”¨ ê´€ë ¨ ì¿¼ë¦¬ ê°œì„  (ìµœì‹  ì •ë³´ ë³´ì¥)
        weather_keywords = ['ë‚ ì”¨', 'ê¸°ì˜¨', 'ì˜¨ë„', 'ìŠµë„', 'ê°•ìˆ˜', 'weather', 'temperature', 'tiempo']
        if any(kw in clean_query.lower() for kw in weather_keywords):
            # ê³¼ê±° ë°ì´í„° ê²€ìƒ‰ ë°©ì§€ ë° ì‹¤ì‹œê°„ ì •ë³´ ê°•ì¡°
            clean_query = clean_query.replace("ê³¼ê±°", "").replace("ì¼ë³„", "").replace("past", "")
            # "í˜„ì¬" ë˜ëŠ” "ì‹¤ì‹œê°„" í‚¤ì›Œë“œ ì¶”ê°€ (ê³¼ê±° ë°ì´í„° í•„í„°ë§)
            if "í˜„ì¬" not in clean_query and "ì‹¤ì‹œê°„" not in clean_query and "current" not in clean_query:
                clean_query = f"í˜„ì¬ ì‹¤ì‹œê°„ {clean_query}"
            logger.info(f"ë‚ ì”¨ ì¿¼ë¦¬ ê°œì„ : '{clean_query}'")
        
        # ê²€ìƒ‰ ìˆ˜í–‰
        search_result = self.search_web(clean_query)
        
        # ì„¸ì…˜ ìƒíƒœ ì €ì¥
        if session_state is not None:
            if "search_contexts" not in session_state:
                session_state.search_contexts = {}
            if "current_context" not in session_state:
                session_state.current_context = None
            
            context_id = str(uuid.uuid4())
            session_state.search_contexts[context_id] = {
                "type": "naver_search",
                "query": clean_query,
                "result": search_result,
                "timestamp": datetime.now().isoformat()
            }
            session_state.current_context = context_id
            
            logger.info(f"âœ… ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {context_id}")
        else:
            logger.error("âŒ ì„¸ì…˜ ìƒíƒœê°€ ì „ë‹¬ë˜ì§€ ì•ŠìŒ!")
        
        # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ì•ˆë‚´ ì¶”ê°€
        enhanced_result = search_result + "\n\nğŸ’¡ ê²€ìƒ‰ ê²°ê³¼ì— ëŒ€í•´ ë” ì§ˆë¬¸í•˜ì‹œë©´ ë‹µë³€í•´ë“œë¦´ê²Œìš”. ì˜ˆë¥¼ ë“¤ì–´:\n"
        enhanced_result += "- 'ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš”ì•½í•´'\n"
        enhanced_result += "- 'ì²« ë²ˆì§¸ ê²°ê³¼ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì¤˜'\n"
        enhanced_result += "- '3ë²ˆì§¸ ë§í¬ ìš”ì•½í•´ì¤˜' (í•´ë‹¹ ìˆœì„œ ì›¹í˜ì´ì§€ ì „ì²´ ë‚´ìš© ìš”ì•½)\n"
        
        return enhanced_result
    
    def get_search_stats(self):
        """ê²€ìƒ‰ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "request_count": self.request_count,
            "daily_limit": self.daily_limit,
            "remaining": self.daily_limit - self.request_count,
            "usage_percentage": round((self.request_count / self.daily_limit) * 100, 2)
        }
    
    def reset_daily_count(self):
        """ì¼ì¼ ì¹´ìš´íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.request_count = 0
        logger.info("Naver API ì¼ì¼ ìš”ì²­ ì¹´ìš´íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
