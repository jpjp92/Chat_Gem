# # Set library imports
# from config.imports import *

# # Set environment variables
# from config.env import *

# # set logging configuration
# import logging
# logger = logging.getLogger(__name__)


# def extract_video_id(url):
#     """ìœ íŠœë¸Œ URLì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ (ì‡¼ì¸  í¬í•¨)"""
#     try:
#         if 'youtu.be/' in url:
#             return url.split('youtu.be/')[1].split('?')[0]
#         elif 'youtube.com/watch' in url:
#             parsed_url = urlparse(url)
#             return parse_qs(parsed_url.query)['v'][0]
#         elif 'youtube.com/embed/' in url:
#             return url.split('embed/')[1].split('?')[0]
#         elif 'youtube.com/shorts/' in url:
#             return url.split('shorts/')[1].split('?')[0]
#         else:
#             return None
#     except:
#         return None

# def is_youtube_url(url):
#     """ìœ íŠœë¸Œ URLì¸ì§€ í™•ì¸ (ì‡¼ì¸  í¬í•¨)"""
#     youtube_domains = ['youtube.com', 'youtu.be', 'www.youtube.com']
#     youtube_patterns = ['/watch', '/shorts/', '/embed/', 'youtu.be/']
#     try:
#         parsed_url = urlparse(url)
#         domain_match = any(domain in parsed_url.netloc for domain in youtube_domains)
#         pattern_match = any(pattern in url for pattern in youtube_patterns)
#         return domain_match and pattern_match
#     except:
#         return False

# def get_youtube_transcript(video_id):
#     """ìœ íŠœë¸Œ ë¹„ë””ì˜¤ì˜ ìë§‰ ê°€ì ¸ì˜¤ê¸°"""
#     try:
#         try:
#             transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko'])
#         except:
#             try:
#                 transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en'])
#             except:
#                 transcript = YouTubeTranscriptApi.get_transcript(video_id)
#         full_text = ' '.join([entry['text'] for entry in transcript])
#         max_chars = 15000
#         if len(full_text) > max_chars:
#             full_text = full_text[:max_chars] + "\n\n... (ìë§‰ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
#         return full_text
#     except Exception as e:
#         logger.error(f"ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
#         return None

# def extract_urls_from_text(text):
#     """í…ìŠ¤íŠ¸ì—ì„œ URLì„ ì¶”ì¶œ"""
#     url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
#     urls = re.findall(url_pattern, text)
#     return urls

# def is_youtube_summarization_request(query):
#     """ìœ íŠœë¸Œ ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸"""
#     urls = extract_urls_from_text(query)
#     if urls:
#         for url in urls:
#             if is_youtube_url(url):
#                 summary_keywords = ['ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¶„ì„', 'í•´ì„', 'ë¦¬ë·°', 'ì •ë³´']
#                 for keyword in summary_keywords:
#                     if keyword in query:
#                         return True, url
#     return False, None


# def is_url_summarization_request(query):
#     """URL ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸ (ìœ íŠœë¸Œ ë° PDF ì œì™¸)"""
#     urls = extract_urls_from_text(query)
#     if urls:
#         for url in urls:
#             if not is_youtube_url(url) and not is_pdf_url(url):
#                 summary_keywords = ['ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¶„ì„', 'í•´ì„', 'ë¦¬ë·°', 'ì •ë³´']
#                 for keyword in summary_keywords:
#                     if keyword in query:
#                         return True, url
#     return False, None

# def fetch_webpage_content(url):
#     """ì¼ë°˜ ì›¹í˜ì´ì§€ HTML ë‚´ìš© ì¶”ì¶œ"""
#     try:
#         headers = {'User-Agent': 'Mozilla/5.0'}
#         response = requests.get(url, headers=headers, timeout=15)
#         response.raise_for_status()
#         soup = BeautifulSoup(response.content, 'html.parser')
#         for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
#             tag.decompose()
#         main_content = soup.find('main') or soup.find('article') or soup.body
#         text = main_content.get_text(strip=True, separator='\n') if main_content else soup.get_text(strip=True, separator='\n')
#         clean_text = '\n'.join(line.strip() for line in text.split('\n') if line.strip())
#         if len(clean_text) > 8000:
#             clean_text = clean_text[:8000] + "\n\n... (ë‚´ìš©ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
#         return clean_text
#     except Exception as e:
#         logger.error(f"ì›¹í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
#         return f"âŒ '{url}' ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"

# def is_pdf_url(url):
#     """PDF URLì¸ì§€ í™•ì¸"""
#     return url.lower().endswith('.pdf') or '/pdf/' in url

# def is_pdf_summarization_request(query):
#     """PDF ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸"""
#     urls = extract_urls_from_text(query)
#     if urls:
#         for url in urls:
#             if is_pdf_url(url):
#                 summary_keywords = ['ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¶„ì„', 'í•´ì„', 'ë¦¬ë·°', 'ì •ë³´']
#                 for keyword in summary_keywords:
#                     if keyword in query:
#                         return True, url
#     return False, None

# def fetch_pdf_text(url, max_chars=8000):
#     """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
#     try:
#         response = requests.get(url, timeout=20)
#         response.raise_for_status()
#         pdf_file = io.BytesIO(response.content)
#         reader = PdfReader(pdf_file)
#         text = ""
#         for page in reader.pages:
#             text += page.extract_text() or ""
#             if len(text) > max_chars:
#                 text = text[:max_chars] + "\n\n... (ë‚´ìš©ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
#                 break
#         metadata = reader.metadata or {}
#         return text.strip(), metadata
#     except Exception as e:
#         return f"âŒ PDF íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}", None

# config/utils.py
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: YouTube, URL, PDF ì²˜ë¦¬ ë° ìš”ì•½ ê´€ë ¨ ê¸°ëŠ¥

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from config.imports import *

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
from config.env import *

# ë¡œê¹… ì„¤ì •
import logging
logger = logging.getLogger(__name__)

def extract_video_id(url):
    """ìœ íŠœë¸Œ URLì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ (ë” ê²¬ê³ í•œ ë²„ì „)"""
    if not url or not isinstance(url, str):
        logger.warning("ìœ íš¨í•˜ì§€ ì•Šì€ URL ì…ë ¥")
        return None
        
    try:
        # URL ì •ê·œí™”
        url = url.strip()
        
        # ë‹¤ì–‘í•œ YouTube URL íŒ¨í„´ ì²˜ë¦¬
        patterns = [
            r'(?:https?://)?(?:www\.)?youtu\.be/([a-zA-Z0-9_-]{11})',
            r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=([a-zA-Z0-9_-]{11})',
            r'(?:https?://)?(?:www\.)?youtube\.com/embed/([a-zA-Z0-9_-]{11})',
            r'(?:https?://)?(?:www\.)?youtube\.com/shorts/([a-zA-Z0-9_-]{11})',
            r'(?:https?://)?(?:m\.)?youtube\.com/watch\?v=([a-zA-Z0-9_-]{11})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                video_id = match.group(1)
                if len(video_id) == 11:  # YouTube ë¹„ë””ì˜¤ IDëŠ” ì •í™•íˆ 11ì
                    logger.info(f"ë¹„ë””ì˜¤ ID ì¶”ì¶œ ì„±ê³µ: {video_id}")
                    return video_id
        
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œë„ ì‹œë„
        if 'youtu.be/' in url:
            video_id = url.split('youtu.be/')[1].split('?')[0].split('&')[0]
        elif 'youtube.com/watch' in url:
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)
            video_id = query_params.get('v', [None])[0]
        elif 'youtube.com/embed/' in url:
            video_id = url.split('embed/')[1].split('?')[0].split('&')[0]
        elif 'youtube.com/shorts/' in url:
            video_id = url.split('shorts/')[1].split('?')[0].split('&')[0]
        else:
            logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” YouTube URL í˜•ì‹: {url}")
            return None
        
        if video_id and len(video_id) == 11:
            logger.info(f"ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ ì„±ê³µ: {video_id}")
            return video_id
        
        logger.warning(f"ë¹„ë””ì˜¤ ID ì¶”ì¶œ ì‹¤íŒ¨: {url}")
        return None
        
    except Exception as e:
        logger.error(f"ë¹„ë””ì˜¤ ID ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)} - URL: {url}")
        return None

def is_youtube_url(url):
    """ìœ íŠœë¸Œ URL í™•ì¸ (ë” ì •í™•í•œ ë²„ì „)"""
    if not url or not isinstance(url, str):
        return False
        
    try:
        url = url.strip().lower()
        youtube_domains = [
            'youtube.com', 'www.youtube.com', 'm.youtube.com',
            'youtu.be', 'www.youtu.be'
        ]
        
        youtube_patterns = [
            '/watch?', '/shorts/', '/embed/', 'youtu.be/'
        ]
        
        # ë„ë©”ì¸ í™•ì¸
        parsed_url = urlparse(url)
        domain_match = any(domain in parsed_url.netloc for domain in youtube_domains)
        
        # íŒ¨í„´ í™•ì¸
        pattern_match = any(pattern in url for pattern in youtube_patterns)
        
        result = domain_match and pattern_match
        logger.debug(f"YouTube URL í™•ì¸: {url[:50]}... -> {result}")
        return result
        
    except Exception as e:
        logger.error(f"YouTube URL í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def get_youtube_transcript(video_id):
    """ìœ íŠœë¸Œ ìë§‰ ê°€ì ¸ì˜¤ê¸° (ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™”)"""
    if not YOUTUBE_TRANSCRIPT_AVAILABLE:
        logger.error("YouTube Transcript APIê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
        return "âŒ YouTube Transcript APIê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    if not video_id or len(video_id) != 11:
        logger.error(f"ì˜ëª»ëœ ë¹„ë””ì˜¤ ID: {video_id}")
        return "âŒ ì˜ëª»ëœ YouTube ë¹„ë””ì˜¤ IDì…ë‹ˆë‹¤."
    
    try:
        logger.info(f"ìë§‰ ì¶”ì¶œ ì‹œì‘: {video_id}")
        
        # ì–¸ì–´ ìš°ì„ ìˆœìœ„: í•œêµ­ì–´ > ì˜ì–´ > ìë™ìƒì„± > ê¸°íƒ€
        language_priorities = [
            ['ko'],           # í•œêµ­ì–´
            ['en'],           # ì˜ì–´
            ['ko', 'en'],     # í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´
            None              # ìë™ ì„ íƒ
        ]
        
        transcript = None
        used_language = None
        
        for languages in language_priorities:
            try:
                if languages:
                    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)
                    used_language = languages[0]
                else:
                    transcript = YouTubeTranscriptApi.get_transcript(video_id)
                    used_language = "auto"
                
                logger.info(f"ìë§‰ ì¶”ì¶œ ì„±ê³µ: {video_id} (ì–¸ì–´: {used_language})")
                break
                
            except Exception as lang_error:
                logger.debug(f"ì–¸ì–´ {languages} ì‹œë„ ì‹¤íŒ¨: {str(lang_error)}")
                continue
        
        if not transcript:
            logger.error(f"ëª¨ë“  ì–¸ì–´ë¡œ ìë§‰ ì¶”ì¶œ ì‹¤íŒ¨: {video_id}")
            return f"âŒ ì´ ì˜ìƒì—ëŠ” ìë§‰ì´ ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nê°€ëŠ¥í•œ ì´ìœ :\nâ€¢ ìë§‰ì´ ë¹„í™œì„±í™”ëœ ì˜ìƒ\nâ€¢ ë¹„ê³µê°œ ë˜ëŠ” ì—°ë ¹ ì œí•œ ì˜ìƒ\nâ€¢ ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¼ ì˜ìƒ\n\në¹„ë””ì˜¤ ID: {video_id}"
        
        # ìë§‰ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        full_text = ""
        for entry in transcript:
            text = entry.get('text', '').strip()
            if text:
                # ìë™ ìƒì„± ìë§‰ì˜ ë…¸ì´ì¦ˆ ì œê±°
                text = re.sub(r'\[.*?\]', '', text)  # [ìŒì•…], [ë°•ìˆ˜] ë“± ì œê±°
                text = re.sub(r'\(.*?\)', '', text)  # (ì›ƒìŒ), (ë°•ìˆ˜) ë“± ì œê±°
                text = text.replace('  ', ' ').strip()
                if text:
                    full_text += text + " "
        
        full_text = full_text.strip()
        
        if not full_text:
            logger.warning(f"ìë§‰ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŒ: {video_id}")
            return "âŒ ìë§‰ì„ ì¶”ì¶œí–ˆì§€ë§Œ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ë° ì •ë³´ ì¶”ê°€
        max_chars = 15000
        if len(full_text) > max_chars:
            full_text = full_text[:max_chars] + "\n\n... (ìë§‰ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
        
        # ìë§‰ ì •ë³´ ì¶”ê°€
        info_header = f"ğŸ“º YouTube ìë§‰ (ì–¸ì–´: {used_language}, ê¸¸ì´: {len(full_text):,}ì)\n\n"
        result = info_header + full_text
        
        logger.info(f"ìë§‰ ì²˜ë¦¬ ì™„ë£Œ: {video_id}, {len(full_text)}ì")
        return result
        
    except Exception as e:
        error_msg = f"ìœ íŠœë¸Œ ìë§‰ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(f"{error_msg} - ë¹„ë””ì˜¤ ID: {video_id}")
        
        # êµ¬ì²´ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì œê³µ
        if "TranscriptsDisabled" in str(e):
            return f"âŒ ì´ ì˜ìƒì€ ìë§‰ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\në¹„ë””ì˜¤ ID: {video_id}"
        elif "VideoUnavailable" in str(e):
            return f"âŒ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\në¹„ë””ì˜¤ ID: {video_id}"
        elif "TooManyRequests" in str(e):
            return f"âŒ ë„ˆë¬´ ë§ì€ ìš”ì²­ìœ¼ë¡œ ì¸í•´ ì¼ì‹œì ìœ¼ë¡œ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        else:
            return f"âŒ {error_msg}\në¹„ë””ì˜¤ ID: {video_id}"

def get_youtube_info_fallback(video_id):
    """YouTube ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìë§‰ ì‹¤íŒ¨ì‹œ ëŒ€ì²´ ë°©ë²•)"""
    if not YT_DLP_AVAILABLE:
        return None
    
    try:
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(f"https://www.youtube.com/watch?v={video_id}", download=False)
            
            title = info.get('title', 'ì œëª© ì—†ìŒ')
            description = info.get('description', '')
            duration = info.get('duration', 0)
            
            # ì„¤ëª…ì—ì„œ ìœ ìš©í•œ ì •ë³´ ì¶”ì¶œ
            if description:
                # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
                if len(description) > 5000:
                    description = description[:5000] + "\n\n... (ì„¤ëª…ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
                
                return f"""
ğŸ“º YouTube ì˜ìƒ ì •ë³´

**ì œëª©:** {title}
**ê¸¸ì´:** {duration//60}ë¶„ {duration%60}ì´ˆ

**ì„¤ëª…:**
{description}
"""
            
        return None
        
    except Exception as e:
        logger.error(f"YouTube ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        return None

def extract_urls_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ URLì„ ì¶”ì¶œ"""
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    urls = re.findall(url_pattern, text)
    return urls

def is_youtube_summarization_request(query):
    """ìœ íŠœë¸Œ ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸"""
    urls = extract_urls_from_text(query)
    if urls:
        for url in urls:
            if is_youtube_url(url):
                summary_keywords = ['ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¶„ì„', 'í•´ì„', 'ë¦¬ë·°', 'ì •ë³´']
                for keyword in summary_keywords:
                    if keyword in query:
                        return True, url
    return False, None

def is_url_summarization_request(query):
    """URL ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸ (ìœ íŠœë¸Œ ë° PDF ì œì™¸)"""
    urls = extract_urls_from_text(query)
    if urls:
        for url in urls:
            if not is_youtube_url(url) and not is_pdf_url(url):
                summary_keywords = ['ìš”ì•½', 'ì •ë¦¬', 'ë‚´ìš©', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¶„ì„', 'í•´ì„', 'ë¦¬ë·°', 'ì •ë³´']
                for keyword in summary_keywords:
                    if keyword in query:
                        return True, url
    return False, None

def fetch_webpage_content(url):
    """ì¼ë°˜ ì›¹í˜ì´ì§€ HTML ë‚´ìš© ì¶”ì¶œ"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            tag.decompose()
        main_content = soup.find('main') or soup.find('article') or soup.body
        text = main_content.get_text(strip=True, separator='\n') if main_content else soup.get_text(strip=True, separator='\n')
        clean_text = '\n'.join(line.strip() for line in text.split('\n') if line.strip())
        if len(clean_text) > 8000:
            clean_text = clean_text[:8000] + "\n\n... (ë‚´ìš©ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
        return clean_text
    except Exception as e:
        logger.error(f"ì›¹í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return f"âŒ '{url}' ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}"

def is_pdf_url(url):
    """PDF URLì¸ì§€ í™•ì¸"""
    return url.lower().endswith('.pdf') or '/pdf/' in url

def fetch_pdf_text(url, max_chars=8000):
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        response = requests.get(url, timeout=20)
        response.raise_for_status()
        pdf_file = io.BytesIO(response.content)
        reader = PdfReader(pdf_file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() or ""
            if len(text) > max_chars:
                text = text[:max_chars] + "\n\n... (ë‚´ìš©ì´ ê¸¸ì–´ì„œ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
                break
        metadata = reader.metadata or {}
        return text.strip(), metadata
    except Exception as e:
        return f"âŒ PDF íŒŒì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}", None