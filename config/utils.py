# # config/utils.py

# # Set library imports
# from config.imports import *

# # Load environment variables
# from config.env import GEMINI_API_KEY

# # set logger
# import logging
# logger = logging.getLogger(__name__)

# def extract_video_id(url: str) -> Optional[str]:
#     """YouTube URLì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ (Shorts í¬í•¨)"""
#     logger.debug(f"Extracting video ID from URL: {url}")
#     patterns = [
#         r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=([^&\n?#]+)',
#         r'(?:https?://)?(?:www\.)?youtube\.com/embed/([^&\n?#]+)',
#         r'(?:https?://)?(?:www\.)?youtube\.com/v/([^&\n?#]+)',
#         r'(?:https?://)?youtu\.be/([^&\n?#]+)',
#         r'(?:https?://)?(?:www\.)?youtube\.com/shorts/([^&\n?#]+)',  # YouTube Shorts íŒ¨í„´ ì¶”ê°€
#     ]
#     for pattern in patterns:
#         match = re.search(pattern, url)
#         if match:
#             video_id = match.group(1)
#             logger.info(f"Successfully extracted video ID: {video_id}")
#             return video_id
#     logger.error(f"Failed to extract video ID from URL: {url}")
#     return None

# def is_youtube_url(url: str) -> bool:
#     """YouTube URLì¸ì§€ í™•ì¸ (Shorts í¬í•¨)"""
#     patterns = [
#         r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=',
#         r'(?:https?://)?(?:www\.)?youtube\.com/embed/',
#         r'(?:https?://)?youtu\.be/',
#         r'(?:https?://)?(?:www\.)?youtube\.com/shorts/',  # YouTube Shorts íŒ¨í„´ ì¶”ê°€
#     ]
#     return any(re.match(pattern, url) for pattern in patterns)

# def is_youtube_summarization_request(text: str) -> tuple[bool, Optional[str]]:
#     """YouTube ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸í•˜ê³  URL ì¶”ì¶œ"""
#     youtube_url = extract_urls_from_text(text)
#     if youtube_url and is_youtube_url(youtube_url[0]):
#         return True, youtube_url[0]
#     return False, None

# def extract_urls_from_text(text: str) -> List[str]:
#     """í…ìŠ¤íŠ¸ì—ì„œ URL ì¶”ì¶œ"""
#     url_pattern = r'https?://[^\s<>"]+|www\.[^\s<>"]+'
#     urls = re.findall(url_pattern, text)
#     return urls

# # def is_url_summarization_request(text: str) -> tuple[bool, Optional[str]]:
# #     """URL ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸í•˜ê³  URL ì¶”ì¶œ"""
# #     urls = extract_urls_from_text(text)
# #     if urls and not is_youtube_url(urls[0]):
# #         return True, urls[0]
# #     return False, None

# def is_url_summarization_request(text: str) -> tuple[bool, Optional[str]]:
#     """URL ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸í•˜ê³  URL ì¶”ì¶œ (PDF ë° YouTube URL ì œì™¸)"""
#     urls = extract_urls_from_text(text)
#     if urls and not is_youtube_url(urls[0]) and not is_pdf_url(urls[0]):
#         return True, urls[0]
#     return False, None


# def is_pdf_url(url):
#     """PDF URLì¸ì§€ í™•ì¸"""
#     return url.lower().endswith('.pdf') or '/pdf/' in url

# def is_pdf_summarization_request(text: str) -> tuple[bool, Optional[str]]:
#     """PDF ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸í•˜ê³  URL ì¶”ì¶œ"""
#     urls = extract_urls_from_text(text)
#     if urls and is_pdf_url(urls[0]):
#         return True, urls[0]
#     return False, None

# def fetch_webpage_content(url: str) -> str:
#     """ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ê°œì„ ëœ ë²„ì „)"""
#     try:
#         session = requests.Session()
#         retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
#         session.mount('https://', HTTPAdapter(max_retries=retries))
#         session.headers.update({
#             'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
#         })
        
#         response = session.get(url, timeout=15)
#         response.raise_for_status()
        
#         from bs4 import BeautifulSoup
#         soup = BeautifulSoup(response.text, 'html.parser')
        
#         # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
#         for script in soup(["script", "style", "nav", "header", "footer", "aside", "advertisement"]):
#             script.decompose()
        
#         # ì£¼ìš” ì»¨í…ì¸  ì˜ì—­ ìš°ì„  ì¶”ì¶œ
#         main_content = None
#         for selector in ['main', 'article', '.content', '.post-content', '.entry-content', '#content']:
#             if content_elem := soup.select_one(selector):
#                 main_content = content_elem
#                 break
        
#         if main_content:
#             text = main_content.get_text(separator=' ', strip=True)
#         else:
#             text = soup.get_text(separator=' ', strip=True)
        
#         # í…ìŠ¤íŠ¸ ì •ë¦¬
#         lines = [line.strip() for line in text.split('\n') if line.strip()]
#         cleaned_text = ' '.join(lines)
        
#         # ê¸¸ì´ ì œí•œ (í† í° ìˆ˜ ê³ ë ¤)
#         return cleaned_text[:20000]  # ê¸°ì¡´ 15000ì—ì„œ 20000ìœ¼ë¡œ ì¦ê°€
        
#     except requests.exceptions.HTTPError as e:
#         if e.response.status_code == 404:
#             logger.error(f"ì›¹í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
#             return f"âŒ ì›¹í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}"
#         logger.error(f"ì›¹í˜ì´ì§€ ì ‘ê·¼ ì˜¤ë¥˜: {str(e)}")
#         return f"âŒ ì›¹í˜ì´ì§€ ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
#     except Exception as e:
#         logger.error(f"ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {str(e)}")
#         return f"âŒ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# # def fetch_webpage_content(url: str) -> str:
# #     """ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
# #     try:
# #         session = requests.Session()
# #         retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
# #         session.mount('https://', HTTPAdapter(max_retries=retries))
# #         session.headers.update({
# #             'Accept-Language': 'ko-KR,ko;q=0.9',
# #             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.82 Safari/537.36'
# #         })
# #         response = session.get(url, timeout=10)
# #         response.raise_for_status()
# #         from bs4 import BeautifulSoup
# #         soup = BeautifulSoup(response.text, 'html.parser')
# #         for script in soup(["script", "style"]):
# #             script.decompose()
# #         text = soup.get_text(separator=' ', strip=True)
# #         return text[:15000]
# #     except Exception as e:
# #         logger.error(f"ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {str(e)}")
# #         return f"âŒ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# def extract_webpage_metadata(url: str, content: str) -> Dict[str, str]:
#     """ì›¹í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
#     try:
#         response = requests.get(url, timeout=10)
#         response.raise_for_status()
        
#         from bs4 import BeautifulSoup
#         soup = BeautifulSoup(response.text, 'html.parser')
        
#         metadata = {
#             "title": "Unknown",
#             "description": "No description available",
#             "author": "Unknown",
#             "published_date": "Unknown",
#             "site_name": "Unknown"
#         }
        
#         # ì œëª© ì¶”ì¶œ
#         if title_tag := soup.find('title'):
#             metadata["title"] = title_tag.get_text().strip()
#         elif h1_tag := soup.find('h1'):
#             metadata["title"] = h1_tag.get_text().strip()
        
#         # ë©”íƒ€ íƒœê·¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
#         meta_tags = soup.find_all('meta')
#         for tag in meta_tags:
#             if tag.get('name') == 'description' or tag.get('property') == 'og:description':
#                 metadata["description"] = tag.get('content', '')[:200]
#             elif tag.get('name') == 'author':
#                 metadata["author"] = tag.get('content', '')
#             elif tag.get('property') == 'og:site_name':
#                 metadata["site_name"] = tag.get('content', '')
#             elif tag.get('name') == 'date' or tag.get('property') == 'article:published_time':
#                 metadata["published_date"] = tag.get('content', '')
        
#         # URLì—ì„œ ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ (fallback)
#         if metadata["site_name"] == "Unknown":
#             from urllib.parse import urlparse
#             parsed_url = urlparse(url)
#             metadata["site_name"] = parsed_url.netloc
        
#         return metadata
        
#     except Exception as e:
#         logger.error(f"ì›¹í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
#         return {
#             "title": "Unknown",
#             "description": "No description available", 
#             "author": "Unknown",
#             "published_date": "Unknown",
#             "site_name": "Unknown"
#         }

# def fetch_pdf_text(url: str) -> tuple[str, Dict, Optional[Dict]]:
#     """PDF ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
#     try:
#         response = requests.get(url, timeout=10)
#         response.raise_for_status()
#         if 'application/pdf' not in response.headers.get('Content-Type', '').lower():
#             logger.error(f"URLì´ PDF í˜•ì‹ì´ ì•„ë‹˜: {url}")
#             return f"âŒ URLì€ PDF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {url}", {}, None
#         pdf_file = io.BytesIO(response.content)
#         reader = PdfReader(pdf_file)
#         text = ""
#         for page in reader.pages:
#             page_text = page.extract_text() or ""
#             text += page_text + " "
#         metadata = reader.metadata or {}
#         sections = None
#         if not text.strip():
#             logger.warning("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#             return "âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", {}, None
#         return text[:15000], metadata, sections
#     except requests.exceptions.HTTPError as e:
#         if e.response.status_code == 404:
#             logger.error(f"PDF URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
#             return f"âŒ PDF URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}", {}, None
#         logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
#         return f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}", {}, None
#     except Exception as e:
#         logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
#         return f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", {}, None

# # def analyze_youtube_with_gemini(video_url: str, user_input: str, model, lang: str) -> Dict[str, Any]:
# #     """Gemini ëª¨ë¸ì„ ì‚¬ìš©í•´ YouTube ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤."""
# #     start_time = time.time()
# #     logger.info(f"Analyzing YouTube video: {video_url}")

# #     try:
# #         question = f"ì´ YouTube ë¹„ë””ì˜¤ë¥¼ {lang == 'ko' and 'í•œêµ­ì–´ë¡œ' or 'in English'} 5ì¤„ ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”: {user_input}"
# #         response = model.generate_content(
# #             [
# #                 {
# #                     "file_data": {
# #                         "file_uri": video_url,
# #                         "mime_type": "video/youtube"
# #                     }
# #                 },
# #                 {"text": question}
# #             ]
# #         )

# #         if response.parts:
# #             result_text = response.text
# #             status = "success"
# #             error = None
# #         else:
# #             result_text = None
# #             status = "failed"
# #             finish_reason = response.candidates[0].finish_reason if response.candidates else 'N/A'
# #             error = f"ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. Finish Reason: {finish_reason}"
# #     except genai.types.BlockedPromptException as e:
# #         result_text = None
# #         status = "blocked"
# #         error = f"í”„ë¡¬í”„íŠ¸ê°€ ì•ˆì „ ì„¤ì •ì— ì˜í•´ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤: {e}"
# #     except Exception as e:
# #         result_text = None
# #         status = "error"
# #         error = f"ë¶„ì„ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}"

# #     processing_time = time.time() - start_time

# #     return {
# #         "video_url": video_url,
# #         "question": user_input,
# #         "summary": result_text,
# #         "status": status,
# #         "error": error,
# #         "processing_time": round(processing_time, 2)
# #     }


# def extract_keywords_from_query(query: str) -> List[str]:
#     """ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
#     # ë¶ˆìš©ì–´ ì œê±°ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
#     stop_words = {'ì´', 'ê·¸', 'ì €', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼', 'ë„', 'ëŠ”', 'ëŠ”', 'the', 'a', 'an', 'and', 'or', 'but'}
#     words = re.findall(r'\b\w+\b', query.lower())
#     return [word for word in words if word not in stop_words and len(word) > 2]

# def estimate_video_length(content: str) -> str:
#     """ë¹„ë””ì˜¤ ê¸¸ì´ ì¶”ì • (ë‚´ìš© ê¸¸ì´ ê¸°ë°˜)"""
#     if not content:
#         return "Unknown"
    
#     word_count = len(content.split())
#     if word_count < 500:
#         return "Short (< 5 min)"
#     elif word_count < 2000:
#         return "Medium (5-20 min)"
#     else:
#         return "Long (> 20 min)"

# def post_process_youtube_summary(summary: str, lang: str) -> str:
#     """ìœ íŠœë¸Œ ìš”ì•½ ê²°ê³¼ í›„ì²˜ë¦¬"""
#     if not summary:
#         return summary
    
#     # ê¸°ë³¸ì ì¸ í˜•ì‹ ì •ë¦¬
#     lines = summary.split('\n')
#     processed_lines = []
    
#     for line in lines:
#         line = line.strip()
#         if line:
#             # ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì œê±°
#             if line not in processed_lines[-3:]:  # ìµœê·¼ 3ì¤„ê³¼ ì¤‘ë³µ ë°©ì§€
#                 processed_lines.append(line)
    
#     return '\n'.join(processed_lines)



# def analyze_youtube_with_gemini(video_url: str, user_input: str, model, lang: str) -> Dict[str, Any]:
#     """Gemini ëª¨ë¸ì„ ì‚¬ìš©í•´ YouTube ë¹„ë””ì˜¤ë¥¼ ê°œì„ ëœ ë°©ì‹ìœ¼ë¡œ ë¶„ì„í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤."""
#     start_time = time.time()
#     logger.info(f"Enhanced YouTube analysis for: {video_url}")

#     try:
#         # ë¹„ë””ì˜¤ ID ì¶”ì¶œ
#         video_id = extract_video_id(video_url)
#         if not video_id:
#             return {
#                 "video_url": video_url,
#                 "question": user_input,
#                 "summary": "âŒ ìœ íš¨í•˜ì§€ ì•Šì€ YouTube URLì…ë‹ˆë‹¤." if lang == 'ko' else "âŒ Invalid YouTube URL.",
#                 "status": "error",
#                 "error": "Invalid YouTube URL",
#                 "processing_time": 0
#             }

#         # ì‚¬ìš©ì ìš”ì²­ ë¶„ì„
#         is_summary_request = any(keyword in user_input.lower() for keyword in 
#                                ['ìš”ì•½', 'ì •ë¦¬', 'summary', 'summarize', 'ì„¤ëª…', 'explain'])
        
#         # í¬ì¸íŠ¸ ê°œìˆ˜ íŒŒì‹±
#         point_count = 5
#         if match := re.search(r'(\d+)ê°œ\s*(í¬ì¸íŠ¸|í•­ëª©|ì¤„)', user_input, re.IGNORECASE):
#             point_count = min(int(match.group(1)), 10)
#         elif match := re.search(r'(\d+)\s*(points|lines)', user_input, re.IGNORECASE):
#             point_count = min(int(match.group(1)), 10)

#         # íŠ¹ì • ì£¼ì œë‚˜ í‚¤ì›Œë“œ ì¶”ì¶œ
#         keywords = extract_keywords_from_query(user_input)
        
#         # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
#         if lang == 'ko':
#             if is_summary_request:
#                 question = f"""ì´ YouTube ë¹„ë””ì˜¤ë¥¼ í•œêµ­ì–´ë¡œ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

# ì‚¬ìš©ì ìš”ì²­: {user_input}

# ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

# ğŸ¬ **ë¹„ë””ì˜¤ ë¶„ì„**

# ğŸ“ **ì£¼ìš” ë‚´ìš©** ({point_count}ê°œ í¬ì¸íŠ¸):
# - í•µì‹¬ í¬ì¸íŠ¸ 1
# - í•µì‹¬ í¬ì¸íŠ¸ 2
# - ...

# ğŸ¯ **í•µì‹¬ ë©”ì‹œì§€**: 
# ë¹„ë””ì˜¤ì˜ ê°€ì¥ ì¤‘ìš”í•œ ë©”ì‹œì§€ë‚˜ ê²°ë¡ 

# ğŸ’¡ **ì£¼ìš” ì¸ì‚¬ì´íŠ¸**:
# íŠ¹ë³„íˆ ì£¼ëª©í•  ë§Œí•œ ë‚´ìš©ì´ë‚˜ ìƒˆë¡œìš´ ì •ë³´

# ğŸ”— **ì¶œì²˜**: {video_url}

# ë¶„ì„ ì§€ì¹¨:
# - ë¹„ë””ì˜¤ì˜ ì£¼ìš” ë‚´ìš©ì„ {point_count}ê°œ í¬ì¸íŠ¸ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
# - ì¤‘ìš”í•œ ë°ì´í„°, í†µê³„, ì‚¬ì‹¤ì´ ìˆë‹¤ë©´ í¬í•¨
# - ë°œí‘œìì˜ í•µì‹¬ ì£¼ì¥ì´ë‚˜ ê²°ë¡ ì„ ëª…í™•íˆ ì œì‹œ
# - ì‹¤ìš©ì ì´ê³  ìœ ìš©í•œ ì •ë³´ ìœ„ì£¼ë¡œ ìš”ì•½
# - ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
# - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”"""
#             else:
#                 question = f"""ì´ YouTube ë¹„ë””ì˜¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

# ì§ˆë¬¸: {user_input}

# ë‹µë³€ ì‹œ ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
# - ë¹„ë””ì˜¤ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì •í™•í•œ ë‹µë³€
# - ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ë°ì´í„°
# - ì‹¤ìš©ì ì¸ ì¡°ì–¸ì´ë‚˜ ì¸ì‚¬ì´íŠ¸
# - ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•œ ëª…í™•í•œ êµ¬ì¡°í™”

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”."""
#         else:
#             if is_summary_request:
#                 question = f"""Please analyze this YouTube video professionally in English.

# User Request: {user_input}

# Please respond in the following format:

# ğŸ¬ **Video Analysis**

# ğŸ“ **Key Points** ({point_count} points):
# - Key point 1
# - Key point 2
# - ...

# ğŸ¯ **Core Message**: 
# The most important message or conclusion of the video

# ğŸ’¡ **Key Insights**:
# Particularly noteworthy content or new information

# ğŸ”— **Source**: {video_url}

# Analysis Guidelines:
# - Systematically organize main content into {point_count} points
# - Include important data, statistics, or facts if present
# - Clearly present the speaker's main arguments or conclusions
# - Focus on practical and useful information
# - Use appropriate emojis for readability
# - Respond only in English"""
#             else:
#                 question = f"""Based on this YouTube video, please answer the following question in English:

# Question: {user_input}

# Please include in your response:
# - Accurate answer based on video content
# - Specific examples or data mentioned
# - Practical advice or insights
# - Clear structure using appropriate emojis

# Respond only in English."""

#         # Gemini API í˜¸ì¶œ
#         response = model.generate_content([
#             {
#                 "file_data": {
#                     "file_uri": video_url,
#                     "mime_type": "video/youtube"
#                 }
#             },
#             {"text": question}
#         ])

#         if response.parts:
#             result_text = response.text
#             status = "success"
#             error = None
            
#             # ê²°ê³¼ í›„ì²˜ë¦¬ - ë” ë‚˜ì€ í˜•ì‹í™”
#             result_text = post_process_youtube_summary(result_text, lang)
            
#         else:
#             result_text = None
#             status = "failed"
#             finish_reason = response.candidates[0].finish_reason if response.candidates else 'N/A'
#             error = f"ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. Finish Reason: {finish_reason}" if lang == 'ko' else f"Response is empty or blocked. Finish Reason: {finish_reason}"

#     except Exception as e:
#         result_text = None
#         status = "error"
#         error_msg = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}" if lang == 'ko' else f"Error during analysis: {e}"
#         error = error_msg
#         logger.error(f"YouTube analysis error: {e}")

#     processing_time = time.time() - start_time

#     return {
#         "video_url": video_url,
#         "question": user_input,
#         "summary": result_text,
#         "status": status,
#         "error": error,
#         "processing_time": round(processing_time, 2)


# config/utils.py

# Set library imports
from config.imports import *

# Load environment variables
from config.env import GEMINI_API_KEY

# set logger
import logging
logger = logging.getLogger(__name__)

def extract_video_id(url: str) -> Optional[str]:
    """YouTube URLì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ (Shorts í¬í•¨)"""
    logger.debug(f"Extracting video ID from URL: {url}")
    patterns = [
        r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=([^&\n?#]+)',
        r'(?:https?://)?(?:www\.)?youtube\.com/embed/([^&\n?#]+)',
        r'(?:https?://)?(?:www\.)?youtube\.com/v/([^&\n?#]+)',
        r'(?:https?://)?youtu\.be/([^&\n?#]+)',
        r'(?:https?://)?(?:www\.)?youtube\.com/shorts/([^&\n?#]+)',  # YouTube Shorts íŒ¨í„´ ì¶”ê°€
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            video_id = match.group(1)
            logger.info(f"Successfully extracted video ID: {video_id}")
            return video_id
    logger.error(f"Failed to extract video ID from URL: {url}")
    return None

def is_youtube_url(url: str) -> bool:
    """YouTube URLì¸ì§€ í™•ì¸ (Shorts í¬í•¨)"""
    patterns = [
        r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=',
        r'(?:https?://)?(?:www\.)?youtube\.com/embed/',
        r'(?:https?://)?youtu\.be/',
        r'(?:https?://)?(?:www\.)?youtube\.com/shorts/',  # YouTube Shorts íŒ¨í„´ ì¶”ê°€
    ]
    return any(re.match(pattern, url) for pattern in patterns)

def is_youtube_summarization_request(text: str) -> tuple[bool, Optional[str]]:
    """YouTube ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸í•˜ê³  URL ì¶”ì¶œ"""
    youtube_url = extract_urls_from_text(text)
    if youtube_url and is_youtube_url(youtube_url[0]):
        return True, youtube_url[0]
    return False, None

def extract_urls_from_text(text: str) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ URL ì¶”ì¶œ"""
    url_pattern = r'https?://[^\s<>"]+|www\.[^\s<>"]+'
    urls = re.findall(url_pattern, text)
    return urls

def is_url_summarization_request(text: str) -> tuple[bool, Optional[str]]:
    """URL ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸í•˜ê³  URL ì¶”ì¶œ (PDF ë° YouTube URL ì œì™¸)"""
    urls = extract_urls_from_text(text)
    if urls and not is_youtube_url(urls[0]) and not is_pdf_url(urls[0]):
        return True, urls[0]
    return False, None

def is_pdf_url(url):
    """PDF URLì¸ì§€ í™•ì¸"""
    return url.lower().endswith('.pdf') or '/pdf/' in url

def is_pdf_summarization_request(text: str) -> tuple[bool, Optional[str]]:
    """PDF ìš”ì•½ ìš”ì²­ì¸ì§€ í™•ì¸í•˜ê³  URL ì¶”ì¶œ"""
    urls = extract_urls_from_text(text)
    if urls and is_pdf_url(urls[0]):
        return True, urls[0]
    return False, None

def fetch_webpage_content(url: str) -> str:
    """ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ê°œì„ ëœ ë²„ì „)"""
    try:
        debug_timings = os.environ.get("STREAMLIT_DEBUG_LOAD_TIMINGS", "0") == "1"
        t0 = time.perf_counter() if debug_timings else None
        
        session = requests.Session()
        retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])
        session.mount('https://', HTTPAdapter(max_retries=retries))
        session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.naver.com/',
            'Cache-Control': 'no-cache'
        })
        
        response = session.get(url, timeout=15, allow_redirects=True)
        response.raise_for_status()
        response.encoding = 'utf-8'  # ì¸ì½”ë”© ëª…ì‹œì  ì„¤ì •
        
        if debug_timings:
            t1 = time.perf_counter()
            logger.info(f"TIMING: fetch_webpage_content GET {url} took {t1 - t0:.4f}s")
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for script in soup(["script", "style", "nav", "header", "footer", "aside", "advertisement", "noscript"]):
            script.decompose()
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ íŠ¹í™” ì²˜ë¦¬
        text = ""
        if "blog.naver.com" in url:
            # ë°©ë²• 1: ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì˜ì—­ (CSS ì„ íƒì)
            selectors_to_try = [
                'div.se-main-container',
                'div#postListBody',
                'div.post-body',
                'div.se-viewer',
                'div.se-component',
                'div[role="main"]',
                'article',
            ]
            
            for selector in selectors_to_try:
                try:
                    post_body = soup.select_one(selector)
                    if post_body:
                        text = post_body.get_text(separator='\n', strip=True)
                        if len(text) > 100:
                            logger.info(f"âœ… ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ (ì…€ë ‰í„°: {selector}): {len(text)} chars")
                            break
                        else:
                            text = ""  # ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ìŒ ì„ íƒì ì‹œë„
                except Exception as e:
                    logger.debug(f"ì„ íƒì ì‹¤íŒ¨ {selector}: {e}")
                    continue
            
            # ë°©ë²• 2: JSON ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ (Naver ë¸”ë¡œê·¸ ê³ ìœ )
            if not text or len(text) < 100:
                try:
                    import json
                    import re
                    
                    # HTMLì—ì„œ JSON ë°ì´í„° ì°¾ê¸°
                    json_pattern = r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>'
                    json_matches = re.findall(json_pattern, response.text, re.DOTALL)
                    
                    for json_str in json_matches:
                        try:
                            json_data = json.loads(json_str)
                            if isinstance(json_data, dict):
                                # articleBody ë˜ëŠ” description ì°¾ê¸°
                                if 'articleBody' in json_data:
                                    text = json_data['articleBody']
                                    logger.info(f"âœ… JSON ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ: {len(text)} chars")
                                    break
                                elif 'description' in json_data:
                                    text = json_data['description']
                                    logger.info(f"âœ… JSON description ì¶”ì¶œ: {len(text)} chars")
                                    break
                        except json.JSONDecodeError:
                            continue
                except Exception as e:
                    logger.debug(f"JSON ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            if not text:
                logger.warning(f"âš ï¸ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ì§€ ëª»í•¨: {url}")
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ê°€ ì•„ë‹ˆê±°ë‚˜ ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
        if not text:
            # ì£¼ìš” ì»¨í…ì¸  ì˜ì—­ ìš°ì„  ì¶”ì¶œ
            main_content = None
            for selector in ['main', 'article', '.content', '.post-content', '.entry-content', '#content', '.post-view', '.blog-content']:
                if content_elem := soup.select_one(selector):
                    main_content = content_elem
                    break
            
            if main_content:
                text = main_content.get_text(separator='\n', strip=True)
            else:
                text = soup.get_text(separator='\n', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        lines = [line.strip() for line in text.split('\n') if line.strip() and len(line.strip()) > 2]
        cleaned_text = '\n'.join(lines)
        
        # ë¹ˆ ë¬¸ìì—´ ì²´í¬
        if not cleaned_text or len(cleaned_text.strip()) < 100:
            logger.warning(f"ì¶”ì¶œëœ ì›¹í˜ì´ì§€ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ: {len(cleaned_text)} chars from {url}")
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ì¸ ê²½ìš° Playwright í´ë°± ì‹œë„
            if "blog.naver.com" in url:
                logger.info(f"Playwright í´ë°± ì‹œë„: {url}")
                playwright_content = fetch_naver_blog_with_playwright(url)
                if playwright_content:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    return playwright_content
            
            return f"âš ï¸ ì›¹í˜ì´ì§€ì—ì„œ ì¶©ë¶„í•œ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URL: {url}\n\nì¶”ì¶œëœ ë‚´ìš©: {cleaned_text[:500]}"
        
        # ê¸¸ì´ ì œí•œ (í† í° ìˆ˜ ê³ ë ¤)
        return cleaned_text[:25000]
        
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            logger.error(f"ì›¹í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
            return f"âŒ ì›¹í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (404): {url}"
        elif e.response.status_code == 403:
            logger.error(f"ì›¹í˜ì´ì§€ ì ‘ê·¼ ê¸ˆì§€: {url}")
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ 403ì¸ ê²½ìš° Playwright ì‹œë„
            if "blog.naver.com" in url:
                logger.info(f"403 ì—ëŸ¬ - Playwright í´ë°± ì‹œë„: {url}")
                playwright_content = fetch_naver_blog_with_playwright(url)
                if playwright_content:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                    return playwright_content
            return f"âŒ ì›¹í˜ì´ì§€ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (403 - ì ‘ê·¼ ê¸ˆì§€): {url}"
        logger.error(f"ì›¹í˜ì´ì§€ ì ‘ê·¼ ì˜¤ë¥˜: {str(e)}")
        return f"âŒ ì›¹í˜ì´ì§€ ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    except Exception as e:
        logger.error(f"ì›¹í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {str(e)}")
        return f"âŒ ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def fetch_naver_blog_with_playwright(url: str) -> str:
    """
    Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë™ì  ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°
    Streamlit Cloud í™˜ê²½ì—ì„œ ë¸Œë¼ìš°ì € ì„¤ì¹˜ê°€ ì•ˆ ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ í´ë°± ì²˜ë¦¬
    """
    # Playwright ë¹„í™œì„±í™” í”Œë˜ê·¸ í™•ì¸
    if os.environ.get("PLAYWRIGHT_DISABLED") == "1":
        logger.warning("ğŸ­ Playwrightê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤ (Streamlit Cloud í™˜ê²½).")
        return ""  # ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ -> ë‹¤ë¥¸ í´ë°±ìœ¼ë¡œ ì§„í–‰
    
    try:
        from playwright.sync_api import sync_playwright
        
        logger.info(f"Playwrightë¡œ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°: {url}")
        
        with sync_playwright() as p:
            # ë¸Œë¼ìš°ì € ì˜µì…˜: ë³´ìˆ˜ì  ì„¤ì •
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-dev-shm-usage', '--single-process']  # Streamlit Cloud ë©”ëª¨ë¦¬ ìµœì í™”
            )
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = context.new_page()
            
            try:
                # í˜ì´ì§€ ë¡œë“œ (JavaScript ì‹¤í–‰)
                page.goto(url, wait_until='networkidle', timeout=30000)
                
                # ì½˜í…ì¸  ì¶”ì¶œ (ëŒ€ê¸° ì—†ì´ ì¦‰ì‹œ ì‹œë„)
                content_html = page.content()
                soup = BeautifulSoup(content_html, 'html.parser')
                
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for elem in soup(['script', 'style', 'nav', 'header', 'footer']):
                    elem.decompose()
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                text = ""
                for selector in ['div.se-main-container', 'div.post-view', 'div#postListBody', 'article']:
                    main_content = soup.select_one(selector)
                    if main_content:
                        text = main_content.get_text(separator='\n', strip=True)
                        break
                
                if not text:
                    text = soup.get_text(separator='\n', strip=True)
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                lines = [line.strip() for line in text.split('\n') if line.strip() and len(line.strip()) > 2]
                cleaned_text = '\n'.join(lines)
                
                logger.info(f"âœ… Playwright ì¶”ì¶œ ì™„ë£Œ: {len(cleaned_text)} chars")
                
                if len(cleaned_text) > 100:
                    return cleaned_text[:25000]
                else:
                    return ""  # ë¹ˆ ê²°ê³¼ëŠ” ë‹¤ë¥¸ í´ë°±ìœ¼ë¡œ ì§„í–‰
                    
            finally:
                context.close()
                browser.close()
                
    except ImportError:
        logger.error("âŒ Playwright ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        return ""
    except FileNotFoundError as e:
        # Chromium ì‹¤í–‰ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        if "headless_shell" in str(e) or "chrome" in str(e).lower():
            logger.warning(f"âš ï¸ Playwright ë¸Œë¼ìš°ì € ë°”ì´ë„ˆë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ìœ¼ë¡œ ë‹¤ìŒ ì‹œë„ì—ì„œ ìŠ¤í‚µ
            os.environ["PLAYWRIGHT_DISABLED"] = "1"
            return ""
        raise
    except Exception as e:
        logger.warning(f"âš ï¸ Playwright ì¶”ì¶œ ì˜¤ë¥˜ (í´ë°±ìœ¼ë¡œ ì§„í–‰): {str(e)}")
        return ""  # ë¹ˆ ê²°ê³¼ ë°˜í™˜ -> requests ê¸°ë°˜ ì¶”ì¶œ ê³„ì† ì‹œë„

def extract_webpage_metadata(url: str, content: str) -> Dict[str, str]:
    """ì›¹í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì´ë¯¸ ê°€ì ¸ì˜¨ content ì‚¬ìš©)"""
    try:
        from bs4 import BeautifulSoup
        
        # ì´ë¯¸ ê°€ì ¸ì˜¨ contentë¥¼ ì‚¬ìš© (ì¶”ê°€ HTTP ìš”ì²­ ë¶ˆí•„ìš”)
        soup = BeautifulSoup(content, 'html.parser')
        
        metadata = {
            "title": "Unknown",
            "description": "No description available",
            "author": "Unknown",
            "published_date": "Unknown",
            "site_name": "Unknown"
        }
        
        # ì œëª© ì¶”ì¶œ
        if title_tag := soup.find('title'):
            metadata["title"] = title_tag.get_text().strip()
        elif h1_tag := soup.find('h1'):
            metadata["title"] = h1_tag.get_text().strip()
        
        # ë©”íƒ€ íƒœê·¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
        meta_tags = soup.find_all('meta')
        for tag in meta_tags:
            if tag.get('name') == 'description' or tag.get('property') == 'og:description':
                metadata["description"] = tag.get('content', '')[:200]
            elif tag.get('name') == 'author':
                metadata["author"] = tag.get('content', '')
            elif tag.get('property') == 'og:site_name':
                metadata["site_name"] = tag.get('content', '')
            elif tag.get('name') == 'date' or tag.get('property') == 'article:published_time':
                metadata["published_date"] = tag.get('content', '')
        
        # URLì—ì„œ ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ (fallback)
        if metadata["site_name"] == "Unknown":
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            metadata["site_name"] = parsed_url.netloc
        
        return metadata
        
    except Exception as e:
        logger.error(f"ì›¹í˜ì´ì§€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return {
            "title": "Unknown",
            "description": "No description available", 
            "author": "Unknown",
            "published_date": "Unknown",
            "site_name": "Unknown"
        }

def fetch_pdf_text(pdf_url: str = None, pdf_file=None) -> tuple[str, Dict, Optional[Dict]]:
    """PDF ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (URL ë˜ëŠ” íŒŒì¼ ì…ë ¥ ì§€ì›)"""
    try:
        debug_timings = os.environ.get("STREAMLIT_DEBUG_LOAD_TIMINGS", "0") == "1"
        t0 = time.perf_counter() if debug_timings else None
        if pdf_url:
            response = requests.get(pdf_url, timeout=10)
            response.raise_for_status()
            if debug_timings:
                t1 = time.perf_counter()
                logger.info(f"TIMING: fetch_pdf_text GET {pdf_url} took {t1 - t0:.4f}s")
            if 'application/pdf' not in response.headers.get('Content-Type', '').lower():
                logger.error(f"URLì´ PDF í˜•ì‹ì´ ì•„ë‹˜: {pdf_url}")
                return f"âŒ URLì€ PDF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤: {pdf_url}", {}, None
            pdf_file = io.BytesIO(response.content)
        elif pdf_file:
            if not isinstance(pdf_file, io.BytesIO):
                pdf_file = io.BytesIO(pdf_file.read())
        else:
            logger.error("PDF URL ë˜ëŠ” íŒŒì¼ì´ ì œê³µë˜ì§€ ì•ŠìŒ")
            return "âŒ PDF URL ë˜ëŠ” íŒŒì¼ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", {}, None

        reader = PdfReader(pdf_file)
        text = ""
        for page in reader.pages:
            page_text = page.extract_text() or ""
            text += page_text + " "
        metadata = reader.metadata or {}
        sections = None
        if not text.strip():
            logger.warning("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", {}, None
        return text[:15000], metadata, sections
    except requests.exceptions.HTTPError as e:
        if pdf_url and e.response.status_code == 404:
            logger.error(f"PDF URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {pdf_url}")
            return f"âŒ PDF URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_url}", {}, None
        logger.error(f"PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return f"âŒ PDF ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}", {}, None
    except Exception as e:
        logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", {}, None

def extract_keywords_from_query(query: str) -> List[str]:
    """ì‚¬ìš©ì ì¿¼ë¦¬ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
    stop_words = {'ì´', 'ê·¸', 'ì €', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼', 'ë„', 'ëŠ”', 'ëŠ”', 'the', 'a', 'an', 'and', 'or', 'but'}
    words = re.findall(r'\b\w+\b', query.lower())
    return [word for word in words if word not in stop_words and len(word) > 2]

def estimate_video_length(content: str) -> str:
    """ë¹„ë””ì˜¤ ê¸¸ì´ ì¶”ì • (ë‚´ìš© ê¸¸ì´ ê¸°ë°˜)"""
    if not content:
        return "Unknown"
    
    word_count = len(content.split())
    if word_count < 500:
        return "Short (< 5 min)"
    elif word_count < 2000:
        return "Medium (5-20 min)"
    else:
        return "Long (> 20 min)"

def post_process_youtube_summary(summary: str, lang: str) -> str:
    """ìœ íŠœë¸Œ ìš”ì•½ ê²°ê³¼ í›„ì²˜ë¦¬"""
    if not summary:
        return summary
    
    lines = summary.split('\n')
    processed_lines = []
    
    for line in lines:
        line = line.strip()
        if line:
            if line not in processed_lines[-3:]:
                processed_lines.append(line)
    
    return '\n'.join(processed_lines)

def analyze_youtube_with_gemini(video_url: str, user_input: str, model, lang: str) -> Dict[str, Any]:
    """Gemini ëª¨ë¸ì„ ì‚¬ìš©í•´ YouTube ë¹„ë””ì˜¤ë¥¼ ë¶„ì„í•˜ê³  transcript, metadata, summaryë¥¼ ëª¨ë‘ ë°˜í™˜í•©ë‹ˆë‹¤."""
    start_time = time.time()
    logger.info(f"Enhanced YouTube analysis for: {video_url}")

    try:
        video_id = extract_video_id(video_url)
        if not video_id:
            return {
                "video_url": video_url,
                "question": user_input,
                "transcript": "",
                "metadata": {},
                "summary": "âŒ ìœ íš¨í•˜ì§€ ì•Šì€ YouTube URLì…ë‹ˆë‹¤." if lang == 'ko' else "âŒ Invalid YouTube URL.",
                "status": "error",
                "error": "Invalid YouTube URL",
                "processing_time": 0
            }

        # 1ë‹¨ê³„: transcriptì™€ metadata ì¶”ì¶œ
        transcript_question = "Please provide the full transcript of this YouTube video along with basic metadata (title, channel). Format your response as:\n\nTITLE: [video title]\nCHANNEL: [channel name]\nTRANSCRIPT:\n[full transcript text]"
        
        transcript_response = model.generate_content([
            {
                "file_data": {
                    "file_uri": video_url,
                    "mime_type": "video/youtube"
                }
            },
            {"text": transcript_question}
        ])

        # transcriptì™€ metadata íŒŒì‹±
        transcript = ""
        metadata = {"title": "Unknown", "channel": "Unknown"}
        
        if transcript_response.parts:
            full_response = transcript_response.text
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            if "TITLE:" in full_response:
                title_match = re.search(r'TITLE:\s*(.+)', full_response)
                if title_match:
                    metadata["title"] = title_match.group(1).strip()
            
            if "CHANNEL:" in full_response:
                channel_match = re.search(r'CHANNEL:\s*(.+)', full_response)
                if channel_match:
                    metadata["channel"] = channel_match.group(1).strip()
            
            # transcript ì¶”ì¶œ
            if "TRANSCRIPT:" in full_response:
                transcript_start = full_response.find("TRANSCRIPT:")
                transcript = full_response[transcript_start + len("TRANSCRIPT:"):].strip()
            else:
                transcript = full_response  # fallback

        # 2ë‹¨ê³„: ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¥¸ ë¶„ì„ (ì„ íƒì )
        summary = ""
        if any(keyword in user_input.lower() for keyword in 
               ['ìš”ì•½', 'ì •ë¦¬', 'summary', 'summarize', 'ì„¤ëª…', 'explain']):
            
            # ê¸°ì¡´ ë¶„ì„ ë¡œì§ ì‚¬ìš©
            is_summary_request = True
            point_count = 5
            if match := re.search(r'(\d+)ê°œ\s*(í¬ì¸íŠ¸|í•­ëª©|ì¤„)', user_input, re.IGNORECASE):
                point_count = min(int(match.group(1)), 10)
            elif match := re.search(r'(\d+)\s*(points|lines)', user_input, re.IGNORECASE):
                point_count = min(int(match.group(1)), 10)

            if lang == 'ko':
                question = f"""ì´ YouTube ë¹„ë””ì˜¤ë¥¼ í•œêµ­ì–´ë¡œ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: {user_input}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

ğŸ“ **ì£¼ìš” ë‚´ìš©** ({point_count}ê°œ í¬ì¸íŠ¸):
- í•µì‹¬ í¬ì¸íŠ¸ 1
- í•µì‹¬ í¬ì¸íŠ¸ 2
- ...

ğŸ¯ **í•µì‹¬ ë©”ì‹œì§€**: 
ë¹„ë””ì˜¤ì˜ ê°€ì¥ ì¤‘ìš”í•œ ë©”ì‹œì§€ë‚˜ ê²°ë¡ 

ğŸ’¡ **ì£¼ìš” ì¸ì‚¬ì´íŠ¸**:
íŠ¹ë³„íˆ ì£¼ëª©í•  ë§Œí•œ ë‚´ìš©ì´ë‚˜ ìƒˆë¡œìš´ ì •ë³´

ë¶„ì„ ì§€ì¹¨:
- ë¹„ë””ì˜¤ì˜ ì£¼ìš” ë‚´ìš©ì„ {point_count}ê°œ í¬ì¸íŠ¸ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
- ì¤‘ìš”í•œ ë°ì´í„°, í†µê³„, ì‚¬ì‹¤ì´ ìˆë‹¤ë©´ í¬í•¨
- ë°œí‘œìì˜ í•µì‹¬ ì£¼ì¥ì´ë‚˜ ê²°ë¡ ì„ ëª…í™•íˆ ì œì‹œ
- ì‹¤ìš©ì ì´ê³  ìœ ìš©í•œ ì •ë³´ ìœ„ì£¼ë¡œ ìš”ì•½
- ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”"""
            else:
                question = f"""Please analyze this YouTube video professionally in English.

User Request: {user_input}

Please respond in the following format:

ğŸ“ **Key Points** ({point_count} points):
- Key point 1
- Key point 2
- ...

ğŸ¯ **Core Message**: 
The most important message or conclusion of the video

ğŸ’¡ **Key Insights**:
Particularly noteworthy content or new information

Analysis Guidelines:
- Systematically organize main content into {point_count} points
- Include important data, statistics, or facts if present
- Clearly present the speaker's main arguments or conclusions
- Focus on practical and useful information
- Respond only in English"""

            analysis_response = model.generate_content([
                {
                    "file_data": {
                        "file_uri": video_url,
                        "mime_type": "video/youtube"
                    }
                },
                {"text": question}
            ])

            if analysis_response.parts:
                summary = analysis_response.text
                summary = post_process_youtube_summary(summary, lang)

        status = "success"
        error = None

    except Exception as e:
        transcript = ""
        metadata = {"title": "Unknown", "channel": "Unknown"}
        summary = ""
        status = "error"
        error_msg = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}" if lang == 'ko' else f"Error during analysis: {e}"
        error = error_msg
        logger.error(f"YouTube analysis error: {e}")

    processing_time = time.time() - start_time

    return {
        "video_url": video_url,
        "question": user_input,
        "transcript": transcript,          # ìƒˆë¡œ ì¶”ê°€
        "metadata": metadata,              # ìƒˆë¡œ ì¶”ê°€
        "summary": summary,
        "status": status,
        "error": error,
        "processing_time": round(processing_time, 2)
    }

def is_image_analysis_request(query, has_images):
    """ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­ì¸ì§€ í™•ì¸ (í•œêµ­ì–´/ì˜ì–´/ìŠ¤í˜ì¸ì–´ ì§€ì›)"""
    if not has_images:
        return False
    
    # í•œêµ­ì–´ í‚¤ì›Œë“œ
    ko_keywords = ['ë¶„ì„', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ë¬´ì—‡', 'ë­', 'ì–´ë–¤', 'ë³´ì—¬ì¤˜', 'ì½ì–´ì¤˜', 'í•´ì„', 'ë¶„ì„í•´ì¤˜']
    
    # ì˜ì–´ í‚¤ì›Œë“œ
    en_keywords = ['analyze', 'describe', 'explain', 'what', 'show', 'read', 'tell', 'see', 'image', 'picture', 'photo']
    
    # ìŠ¤í˜ì¸ì–´ í‚¤ì›Œë“œ ì¶”ê°€
    es_keywords = [
        'analizar', 'describir', 'explicar', 'quÃ©', 'mostrar', 'leer', 'decir', 'ver', 
        'imagen', 'foto', 'picture', 'muestra', 'enseÃ±a', 'dice', 'contiene',
        'puedes', 'podrÃ­as', 'ayuda', 'favor'
    ]
    
    # ëª¨ë“  í‚¤ì›Œë“œ í†µí•©
    all_keywords = ko_keywords + en_keywords + es_keywords
    
    return any(keyword in query.lower() for keyword in all_keywords)

def is_pdf_analysis_request(query, has_pdf):
    """PDF ë¶„ì„ ìš”ì²­ì¸ì§€ í™•ì¸ (í•œêµ­ì–´/ì˜ì–´/ìŠ¤í˜ì¸ì–´ ì§€ì›)"""
    if not has_pdf and not is_pdf_summarization_request(query)[0]:
        return False
    
    # ë‹¤êµ­ì–´ ë¶„ì„ í‚¤ì›Œë“œ
    analysis_keywords = [
        # í•œêµ­ì–´
        'ìš”ì•½', 'ë¶„ì„', 'ì„¤ëª…', 'ì•Œë ¤ì¤˜', 'ì •ë¦¬',
        # ì˜ì–´  
        'summarize', 'analyze', 'explain', 'describe', 'tell',
        # ìŠ¤í˜ì¸ì–´
        'resumir', 'analizar', 'explicar', 'describir', 'decir',
        'mostrar', 'ayuda', 'puedes', 'podrÃ­as'
    ]
    
    return any(keyword in query.lower() for keyword in analysis_keywords)

def create_summary(text: str, target_length: int = 400) -> str:
    """ê¸€ììˆ˜ ê¸°ì¤€ ìš”ì•½ ìƒì„± (ìµœì¢… í´ë°±ìš©)"""
    sentences = re.split(r'[.!?]\s+', text)
    sentences = [s.strip() for s in sentences if s.strip() and len(s) > 15]
    if not sentences:
        return text[:target_length] + "..." if len(text) > target_length else text
    sentence_scores = []
    for i, sentence in enumerate(sentences):
        length_score = len(sentence.split())
        position_score = max(0, 10 - i * 0.5)
        total_score = length_score + position_score
        sentence_scores.append((total_score, sentence))
    sentence_scores.sort(reverse=True)
    summary = ""
    for score, sentence in sentence_scores:
        test_summary = summary + sentence + ". "
        if len(test_summary) <= target_length:
            summary = test_summary
        elif len(summary) < 100:
            remaining = target_length - len(summary) - 3
            if remaining > 50:
                summary += sentence[:remaining] + "..."
                break
        else:
            break
    if len(summary) < 50:
        summary = text[:target_length-3] + "..."
    return summary.strip()